{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, make_scorer,classification_report,confusion_matrix\n",
    "\n",
    "def classificator_score(clasificador, entrenamiento,y_test):\n",
    "    print('Best parameters:\\n')\n",
    "    best_parameters = clasificador.best_estimator_.get_params()\n",
    "    for param in sorted(best_parameters.keys()):\n",
    "        print((param, best_parameters[param]))\n",
    "    prediction = clasificador.predict(entrenamiento)\n",
    "    matew = matthews_corrcoef(y_test, prediction)\n",
    "    print('Matthews correlation coefficienr:', matew)\n",
    "    #print(\"Confusion Matrix: \\n \", confusion_matrix(y_test, prediction))\n",
    "    return prediction\n",
    "\n",
    "def preporcessing_data(type_analyzer,ngram_size,feature_chosed):\n",
    "    postables_local = open(\"../../dataset/TablesPOS.txt\").readlines()\n",
    "    possentenes_local = open(\"../../dataset/NoTablesPOS.txt\").readlines()\n",
    "    poscomplete_local = postables_local + possentenes_local\n",
    "\n",
    "    if feature_chosed != ['all']:\n",
    "        new_sentence = []\n",
    "        for sentence_num in range(len(poscomplete_local)):\n",
    "            tokens = poscomplete_local[sentence_num].split(' ')\n",
    "            new_words = []\n",
    "            for word_num in tokens:\n",
    "                if word_num in feature_chosed:\n",
    "                    new_words.append(word_num)\n",
    "            new_sentence.append(' '.join(new_words))\n",
    "    else:\n",
    "        new_sentence = poscomplete_local\n",
    "\n",
    "\n",
    "    class_label_local = []\n",
    "    for i in range(len(new_sentence)):\n",
    "        if i < len(postables_local):\n",
    "            class_label_local.append('TABLES')\n",
    "        else:\n",
    "            class_label_local.append('NONE TABLES')\n",
    "\n",
    "    fun_x_train, fun_x_test, fun_y_train, fun_y_test = train_test_split(new_sentence, class_label_local, train_size=0.85, test_size=0.15)\n",
    "\n",
    "    tdifvectorizer = TfidfVectorizer(analyzer = type_analyzer,ngram_range=ngram_size)\n",
    "    x_train_fun = tdifvectorizer.fit_transform(fun_x_train)\n",
    "    x_test_fun = tdifvectorizer.transform(fun_x_test)\n",
    "\n",
    "    variance_selector = VarianceThreshold()\n",
    "    variance_selector = variance_selector.fit(x_train_fun,x_test_fun)\n",
    "    x_train_fun = variance_selector.transform(x_train_fun)\n",
    "    x_test_fun = variance_selector.transform(x_test_fun)\n",
    "\n",
    "    return  x_train_fun, x_test_fun, fun_y_train, fun_y_test\n",
    "\n",
    "\n",
    "def run_classifier_grid(x_train, x_test, y_train, y_test ,scorer_fun):\n",
    "    f1scores = []\n",
    "    mathewscores = []\n",
    "    print(\"\\n----------------------------------- Random Forest -----------------------------------------\")\n",
    "    rand_forest_classifier = RandomForestClassifier()\n",
    "    rand_forest_param_grid = {'n_estimators': [100, 150, 200, 300],\n",
    "                              'bootstrap': [True, False],\n",
    "                              'criterion': [\"gini\", \"entropy\"],\n",
    "                              'class_weight': ['balanced', None]}\n",
    "\n",
    "    random_forest = model_selection.RandomizedSearchCV(rand_forest_classifier, rand_forest_param_grid, cv=crossV,\n",
    "                                                       n_jobs=jobs, scoring=scorer_fun, verbose=0)\n",
    "    random_forest.fit(x_train, y_train)\n",
    "    rand_forest_predict = classificator_score(random_forest, x_test, y_test)\n",
    "    score = f1_score(y_test, rand_forest_predict, pos_label='TABLES', average='binary')\n",
    "    matw = matthews_corrcoef(y_test, rand_forest_predict)\n",
    "    f1scores.append(score)\n",
    "    mathewscores.append(matw)\n",
    "    print(classification_report(y_test, rand_forest_predict))\n",
    "\n",
    "    print(\"\\n----------------------------------- SGDClassifier  -----------------------------------------\")\n",
    "    sgdc_classifier = SGDClassifier(loss='log')\n",
    "    sgdc_param_grid = {'alpha': [10 ** (-x) for x in range(7)],\n",
    "                       'penalty': ['elasticnet', 'l1', 'l2'],\n",
    "                       'l1_ratio': [0.15, 0.25, 0.5, 0.75],\n",
    "                       'class_weight': ['balanced', None], }\n",
    "\n",
    "    sgdc_model = model_selection.RandomizedSearchCV(sgdc_classifier, sgdc_param_grid, cv=crossV, n_iter=50, n_jobs=jobs,\n",
    "                                                    scoring=scorer_fun, verbose=0)\n",
    "    sgdc_model.fit(x_train, y_train)\n",
    "    sgdc_predict = classificator_score(sgdc_model, x_test, y_test)\n",
    "    score = f1_score(y_test, sgdc_predict, pos_label='TABLES', average='binary')\n",
    "    matw = matthews_corrcoef(y_test, sgdc_predict)\n",
    "    f1scores.append(score)\n",
    "    mathewscores.append(matw)\n",
    "    print(classification_report(y_test, sgdc_predict))\n",
    "\n",
    "    print(\"\\n-------------------- Radial Basis Function Support Vector Machine  --------------------\")\n",
    "    svm = SVC()\n",
    "    svm_param_grid = {'C': np.arange(1, 50, 0.5),\n",
    "                      'gamma': np.arange(0.01, 1.01, 0.1),\n",
    "                      'kernel': ['rbf'], 'class_weight': ['balanced', None], }\n",
    "\n",
    "    # The number of iterations was reduced in this example in order to save computational time,\n",
    "    svm_rbf = model_selection.RandomizedSearchCV(svm, svm_param_grid, n_iter=100, cv=crossV, n_jobs=-1,\n",
    "                                                 scoring=scorer_fun, verbose=0)\n",
    "    svm_rbf.fit(x_train, y_train)\n",
    "    svm_predict = classificator_score(svm_rbf, x_test, y_test)\n",
    "    score = f1_score(y_test, svm_predict, pos_label='TABLES', average='binary')\n",
    "    matw = matthews_corrcoef(y_test, svm_predict)\n",
    "    f1scores.append(score)\n",
    "    mathewscores.append(matw)\n",
    "    print(classification_report(y_test, svm_predict))\n",
    "    return f1scores, mathewscores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the best parameter configuration obtained during grid search.\n",
    "#### Here we foucus on hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------- Random Forest -----------------------------------------\n",
      "Best parameters:\n",
      "\n",
      "('bootstrap', False)\n",
      "('ccp_alpha', 0.0)\n",
      "('class_weight', 'balanced')\n",
      "('criterion', 'gini')\n",
      "('max_depth', None)\n",
      "('max_features', 'auto')\n",
      "('max_leaf_nodes', None)\n",
      "('max_samples', None)\n",
      "('min_impurity_decrease', 0.0)\n",
      "('min_samples_leaf', 1)\n",
      "('min_samples_split', 2)\n",
      "('min_weight_fraction_leaf', 0.0)\n",
      "('n_estimators', 100)\n",
      "('n_jobs', None)\n",
      "('oob_score', False)\n",
      "('random_state', None)\n",
      "('verbose', 0)\n",
      "('warm_start', False)\n",
      "Matthews correlation coefficienr: 0.8610203001091269\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " NONE TABLES       1.00      1.00      1.00      6811\n",
      "      TABLES       0.96      0.77      0.86        62\n",
      "\n",
      "    accuracy                           1.00      6873\n",
      "   macro avg       0.98      0.89      0.93      6873\n",
      "weighted avg       1.00      1.00      1.00      6873\n",
      "\n",
      "\n",
      "----------------------------------- SGDClassifier  -----------------------------------------\n",
      "Best parameters:\n",
      "\n",
      "('alpha', 1e-06)\n",
      "('average', False)\n",
      "('class_weight', None)\n",
      "('early_stopping', False)\n",
      "('epsilon', 0.1)\n",
      "('eta0', 0.0)\n",
      "('fit_intercept', True)\n",
      "('l1_ratio', 0.5)\n",
      "('learning_rate', 'optimal')\n",
      "('loss', 'log')\n",
      "('max_iter', 1000)\n",
      "('n_iter_no_change', 5)\n",
      "('n_jobs', None)\n",
      "('penalty', 'l1')\n",
      "('power_t', 0.5)\n",
      "('random_state', None)\n",
      "('shuffle', True)\n",
      "('tol', 0.001)\n",
      "('validation_fraction', 0.1)\n",
      "('verbose', 0)\n",
      "('warm_start', False)\n",
      "Matthews correlation coefficienr: 0.8643733227370087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " NONE TABLES       1.00      1.00      1.00      6811\n",
      "      TABLES       0.91      0.82      0.86        62\n",
      "\n",
      "    accuracy                           1.00      6873\n",
      "   macro avg       0.95      0.91      0.93      6873\n",
      "weighted avg       1.00      1.00      1.00      6873\n",
      "\n",
      "\n",
      "-------------------- Radial Basis Function Support Vector Machine  --------------------\n",
      "Best parameters:\n",
      "\n",
      "('C', 16.0)\n",
      "('break_ties', False)\n",
      "('cache_size', 200)\n",
      "('class_weight', None)\n",
      "('coef0', 0.0)\n",
      "('decision_function_shape', 'ovr')\n",
      "('degree', 3)\n",
      "('gamma', 0.6100000000000001)\n",
      "('kernel', 'rbf')\n",
      "('max_iter', -1)\n",
      "('probability', False)\n",
      "('random_state', None)\n",
      "('shrinking', True)\n",
      "('tol', 0.001)\n",
      "('verbose', False)\n",
      "Matthews correlation coefficienr: 0.8860690249643604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " NONE TABLES       1.00      1.00      1.00      6811\n",
      "      TABLES       0.89      0.89      0.89        62\n",
      "\n",
      "    accuracy                           1.00      6873\n",
      "   macro avg       0.94      0.94      0.94      6873\n",
      "weighted avg       1.00      1.00      1.00      6873\n",
      "\n",
      "Run #1\n",
      "F1 Score: [0.8571428571428571, 0.864406779661017, 0.8870967741935484]\n",
      "Matthews Correlation: [0.8610203001091269, 0.8643733227370087, 0.8860690249643604]\n",
      "Vectorization:  char \n",
      "N-gram range:  (2, 5) \n",
      "Scorer:  weighted \n",
      "Selected features all_pos \n",
      "Feature selection None \n",
      "Oversampling: None\n",
      "########################################################\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------- Random Forest -----------------------------------------\n",
      "Best parameters:\n",
      "\n",
      "('bootstrap', False)\n",
      "('ccp_alpha', 0.0)\n",
      "('class_weight', 'balanced')\n",
      "('criterion', 'entropy')\n",
      "('max_depth', None)\n",
      "('max_features', 'auto')\n",
      "('max_leaf_nodes', None)\n",
      "('max_samples', None)\n",
      "('min_impurity_decrease', 0.0)\n",
      "('min_samples_leaf', 1)\n",
      "('min_samples_split', 2)\n",
      "('min_weight_fraction_leaf', 0.0)\n",
      "('n_estimators', 150)\n",
      "('n_jobs', None)\n",
      "('oob_score', False)\n",
      "('random_state', None)\n",
      "('verbose', 0)\n",
      "('warm_start', False)\n",
      "Matthews correlation coefficienr: 0.7868833351002459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " NONE TABLES       1.00      1.00      1.00      6807\n",
      "      TABLES       0.98      0.64      0.77        66\n",
      "\n",
      "    accuracy                           1.00      6873\n",
      "   macro avg       0.99      0.82      0.88      6873\n",
      "weighted avg       1.00      1.00      1.00      6873\n",
      "\n",
      "\n",
      "----------------------------------- SGDClassifier  -----------------------------------------\n",
      "Best parameters:\n",
      "\n",
      "('alpha', 1e-06)\n",
      "('average', False)\n",
      "('class_weight', None)\n",
      "('early_stopping', False)\n",
      "('epsilon', 0.1)\n",
      "('eta0', 0.0)\n",
      "('fit_intercept', True)\n",
      "('l1_ratio', 0.5)\n",
      "('learning_rate', 'optimal')\n",
      "('loss', 'log')\n",
      "('max_iter', 1000)\n",
      "('n_iter_no_change', 5)\n",
      "('n_jobs', None)\n",
      "('penalty', 'l1')\n",
      "('power_t', 0.5)\n",
      "('random_state', None)\n",
      "('shuffle', True)\n",
      "('tol', 0.001)\n",
      "('validation_fraction', 0.1)\n",
      "('verbose', 0)\n",
      "('warm_start', False)\n",
      "Matthews correlation coefficienr: 0.8374760495392828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " NONE TABLES       1.00      1.00      1.00      6807\n",
      "      TABLES       0.91      0.77      0.84        66\n",
      "\n",
      "    accuracy                           1.00      6873\n",
      "   macro avg       0.95      0.89      0.92      6873\n",
      "weighted avg       1.00      1.00      1.00      6873\n",
      "\n",
      "\n",
      "-------------------- Radial Basis Function Support Vector Machine  --------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = 0\n",
    "jobs = -1\n",
    "crossV = 5\n",
    "all_matew = {}\n",
    "all_scores = {}\n",
    "\n",
    "vectorization = ['char']\n",
    "feature_selection =  ['None']\n",
    "type_scorer = ['weighted']\n",
    "n_gram_range = {'char':[(2,5),(3,5)]}\n",
    "feature_type = {'all_pos':['all']}\n",
    "\n",
    "\n",
    "for analyzer in vectorization:\n",
    "    for ngram in n_gram_range[analyzer]:\n",
    "        for scorer in type_scorer:\n",
    "\n",
    "            if scorer == 'weighted':\n",
    "                run_scorer = make_scorer(f1_score, average='weighted')\n",
    "            else:\n",
    "                run_scorer = make_scorer(f1_score, labels=['NONE TABLES','TABLES'],\n",
    "                                         average = 'binary',pos_label = 'TABLES')\n",
    "\n",
    "            for feature in feature_type.keys():\n",
    "                which_features = feature_type[feature]\n",
    "                x_train_ds, x_test_ds, y_train_labels, y_test_labels = preporcessing_data(analyzer,ngram,which_features)\n",
    "\n",
    "                for selection in feature_selection:\n",
    "                    run += 1\n",
    "                    if (selection != 'None') and (feature == 'all_pos'):\n",
    "                        selector = SelectPercentile(chi2,percentile=selection)\n",
    "                        x_train_ds = selector.fit_transform(x_train_ds,y_train_labels)\n",
    "                        x_test_ds = selector.transform(x_test_ds)\n",
    "\n",
    "\n",
    "                    score_f1,score_mathew = run_classifier_grid(x_train_ds, x_test_ds, y_train_labels, y_test_labels, run_scorer)\n",
    "                    print(f'Run #{run}')\n",
    "                    print(f'F1 Score: {score_f1}')\n",
    "                    print(f'Matthews Correlation: {score_mathew}')\n",
    "                    print('Vectorization: ',analyzer,'\\nN-gram range: ',ngram,'\\nScorer: ',scorer,'\\nSelected features',feature,'\\nFeature selection',selection,'\\nOversampling: None')\n",
    "                    print('########################################################\\n\\n')\n",
    "                    all_scores[f'Run{run}_f1'] = score_f1\n",
    "                    all_matew[f'Run{run}_Matthew'] = score_mathew\n",
    "\n",
    "\n",
    "\n",
    "# Note that even in the OO-style, we use `.pyplot.figure` to create the Figure.\n",
    "fig, ax = plt.subplots(figsize=(10, 7), layout='constrained',)\n",
    "ax.plot(all_scores.keys(),all_scores.values(), label=['Random Forest','SGD Classifier','SVM'],linewidth=3)  # Plot some data on the axes.\n",
    "ax.set_ylabel('F1-Score Positive Class',fontsize=13)  # Add a y-label to the axes.\n",
    "ax.set_title(\"Scores using word n-grams size (1-3)\",fontsize=15)  # Add a title to the axes.\n",
    "plt.xticks(fontsize = 13)\n",
    "plt.yticks(fontsize = 13)\n",
    "ax.legend(fontsize=13)  # Add a legend.\n",
    "plt.savefig('f1score')\n",
    "plt.show()\n",
    "# Note that even in the OO-style, we use `.pyplot.figure` to create the Figure.\n",
    "fig, ax = plt.subplots(figsize=(10, 7), layout='constrained',)\n",
    "ax.plot(all_matew.keys(),all_matew.values(), label=['Random Forest','SGD Classifier','SVM'],linewidth=3)  # Plot some data on the axes.\n",
    "ax.set_ylabel('Matthews correlation coefficient',fontsize=13)  # Add a y-label to the axes.\n",
    "ax.set_title(\"Scores using word n-grams size (1-3)\",fontsize=15)  # Add a title to the axes.\n",
    "plt.xticks(fontsize = 13)\n",
    "plt.yticks(fontsize = 13)\n",
    "ax.legend(fontsize=13)\n",
    "plt.savefig('mathew')\n",
    "plt.show()# Add a legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Features\n",
    "#### Top 30 most important features on our dataset. Features were generated by character-ngrams of size (2,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>N-gram Size</th>\n",
       "      <th>Tags considered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>( . n)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>( . nn)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>( : :)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>( : : )</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>( cd c)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>( cd d)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>( cd s)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>( nn :)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>( nn d)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>( s)</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>( sy)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>( sym)</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>( sym )</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(. n)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(. nn)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(. nn )</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(: :)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(: : )</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(: : n)</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(cd c)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(cd cd)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(cd s)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(cd sy)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(d : :)</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(d c)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(d cd)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(d cd )</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(d s)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(d sy)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(d sym)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(m )</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(m c)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(m cd)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(m cd )</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(m h)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(m hy)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(m hyp)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(n . n)</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(n : :)</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(sy)</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(sym)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(sym )</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(sym c)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(sym h)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(ym)</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(ym )</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(ym c)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>(ym cd)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(ym h)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>(ym hy)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature  N-gram Size  Tags considered\n",
       "1    ( . n)            4                2\n",
       "2   ( . nn)            5                2\n",
       "3    ( : :)            4                2\n",
       "4   ( : : )            5                2\n",
       "5   ( cd c)            5                2\n",
       "6   ( cd d)            5                2\n",
       "7   ( cd s)            5                2\n",
       "8   ( nn :)            5                2\n",
       "9   ( nn d)            5                2\n",
       "10     ( s)            2                1\n",
       "11    ( sy)            3                1\n",
       "12   ( sym)            4                1\n",
       "13  ( sym )            5                1\n",
       "14    (. n)            3                2\n",
       "15   (. nn)            4                2\n",
       "16  (. nn )            5                2\n",
       "17    (: :)            3                2\n",
       "18   (: : )            4                2\n",
       "19  (: : n)            5                3\n",
       "20   (cd c)            4                2\n",
       "21  (cd cd)            5                2\n",
       "22   (cd s)            4                2\n",
       "23  (cd sy)            5                2\n",
       "24  (d : :)            5                3\n",
       "25    (d c)            3                2\n",
       "26   (d cd)            4                2\n",
       "27  (d cd )            5                2\n",
       "28    (d s)            3                2\n",
       "29   (d sy)            4                2\n",
       "30  (d sym)            5                2\n",
       "31     (m )            2                1\n",
       "32    (m c)            3                2\n",
       "33   (m cd)            4                2\n",
       "34  (m cd )            5                2\n",
       "35    (m h)            3                2\n",
       "36   (m hy)            4                2\n",
       "37  (m hyp)            5                2\n",
       "38  (n . n)            5                3\n",
       "39  (n : :)            5                3\n",
       "40     (sy)            2                1\n",
       "41    (sym)            3                1\n",
       "42   (sym )            4                1\n",
       "43  (sym c)            5                2\n",
       "44  (sym h)            5                2\n",
       "45     (ym)            2                1\n",
       "46    (ym )            3                1\n",
       "47   (ym c)            4                2\n",
       "48  (ym cd)            5                2\n",
       "49   (ym h)            4                2\n",
       "50  (ym hy)            5                2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "postables = open(\"../../dataset/TablesPOS.txt\").readlines()\n",
    "possentenes = open(\"../../dataset/NoTablesPOS.txt\").readlines()\n",
    "poscomplete = postables + possentenes\n",
    "\n",
    "class_label = []\n",
    "len_instances = []\n",
    "for i in range(len(poscomplete)):\n",
    "    if i < len(postables):\n",
    "        class_label.append('TABLES')\n",
    "        len_instances.append(len(poscomplete[i].replace('\\n','').split(' ')))\n",
    "    else:\n",
    "        class_label.append('NONE TABLES')\n",
    "        len_instances.append(len(poscomplete[i].replace('\\n','').split(' ')))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(poscomplete, class_label, train_size=0.80, test_size=0.20)\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer = 'char',ngram_range=(2,5))\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "\n",
    "selector = SelectPercentile(chi2,percentile=1)\n",
    "x_train = selector.fit_transform(x_train,y_train)\n",
    "x_test = selector.transform(x_test)\n",
    "\n",
    "rank = {}\n",
    "length = []\n",
    "words = []\n",
    "all_features = vectorizer.get_feature_names()\n",
    "best_features = list(selector.get_support(indices=True))\n",
    "for i,top in enumerate(best_features):\n",
    "    rank[f'#{i + 1}'] = ('(' + all_features[top] + ')')\n",
    "    length.append(len(all_features[top]))\n",
    "    size = [n for n in all_features[top].split(' ')]\n",
    "    if size[0] == '' and size[-1] == '':\n",
    "        words.append(len(size)-2)\n",
    "    elif size[0] == '' or size[-1] == '':\n",
    "        words.append(len(size)-1)\n",
    "    else:\n",
    "        words.append(len(size))\n",
    "\n",
    "best_df = pd.DataFrame.from_dict(rank.values())\n",
    "best_df['n-gram size'] = length\n",
    "best_df.columns = ['Feature','N-gram Size']\n",
    "best_df.index = [n for n in range(1,30)]\n",
    "best_df['Tags considered'] = words[:30]\n",
    "best_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############################################################################################################\n",
      "\n",
      "Vectorizing...\n",
      "Done!\n",
      "#############################################################################################################\n",
      "Training Classifier...\n",
      "Done!\n",
      "\n",
      "\n",
      "#############################################################################################################\n",
      "\n",
      "Classifier Scores:\n",
      "\n",
      "Precision (f1 Score):\n",
      "Negative Class: % 0.9990453110082985 \n",
      "Positve Class: % 0.8992248062015504\n",
      "\n",
      "Confusion Matrix: \n",
      "  [[6802    7]\n",
      " [   6   58]] \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NONE TABLE       1.00      1.00      1.00      6809\n",
      "       TABLE       0.89      0.91      0.90        64\n",
      "\n",
      "    accuracy                           1.00      6873\n",
      "   macro avg       0.95      0.95      0.95      6873\n",
      "weighted avg       1.00      1.00      1.00      6873\n",
      "\n",
      "\n",
      "\n",
      "Aditional Info:\n",
      "Number of suport Vectors for each class:  [386 210]\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------- Random Forest -----------------------------------------\n",
      "Done!\n",
      "\n",
      "#############################################################################################################\n",
      "Classifier Scores:\n",
      "\n",
      "Precision (f1 Score):\n",
      "Negative Class: % 0.9986795774647887 \n",
      "Positve Class: % 0.8421052631578947\n",
      "\n",
      "Confusion Matrix: \n",
      "  [[6807    2]\n",
      " [  16   48]] \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NONE TABLE       1.00      1.00      1.00      6809\n",
      "       TABLE       0.96      0.75      0.84        64\n",
      "\n",
      "    accuracy                           1.00      6873\n",
      "   macro avg       0.98      0.87      0.92      6873\n",
      "weighted avg       1.00      1.00      1.00      6873\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------- SGDClassifier  -----------------------------------------\n",
      "Done!\n",
      "\n",
      "\n",
      "\n",
      "#############################################################################################################\n",
      "\n",
      "Classifier Scores:\n",
      "\n",
      "Precision (f1 Score):\n",
      "Negative Class: % 0.9991190720892673 \n",
      "Positve Class: % 0.9032258064516129\n",
      "\n",
      "Confusion Matrix: \n",
      "  [[6805    4]\n",
      " [   8   56]] \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NONE TABLE       1.00      1.00      1.00      6809\n",
      "       TABLE       0.93      0.88      0.90        64\n",
      "\n",
      "    accuracy                           1.00      6873\n",
      "   macro avg       0.97      0.94      0.95      6873\n",
      "weighted avg       1.00      1.00      1.00      6873\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "    # VECTORIZATION AND DATA PREPROCESSING\n",
    "\n",
    "## Directory and files to save trained model\n",
    "### Storage directory of all required objects\n",
    "\n",
    "\n",
    "## Extract POS of cases files. Uncoment if the input files are the ones returned by CoreNLP preprocessing.\n",
    "#POS_extraction()\n",
    "\n",
    "\n",
    "## Vectorize features\n",
    "print(\"\\n#############################################################################################################\")\n",
    "print('\\nVectorizing...')\n",
    "\n",
    "all_matew = {}\n",
    "all_scores = {}\n",
    "postables = open(\"../../dataset/TablesPOS.txt\").readlines()\n",
    "possentenes = open(\"../../dataset/NoTablesPOS.txt\").readlines()\n",
    "whole_text = postables + possentenes\n",
    "f1scores = []\n",
    "mathewscores = []\n",
    "\n",
    "## Asign a positive or negative value.\n",
    "class_label = []\n",
    "for x in range(len(whole_text)):\n",
    "    if x < len(postables):\n",
    "        class_label.append('TABLE')\n",
    "    else:\n",
    "        class_label.append('NONE TABLE')\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "\n",
    "## Split data for train and test the model\n",
    "print(\"#############################################################################################################\")\n",
    "print(\"Training Classifier...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(whole_text, class_label, train_size=0.85, test_size=0.15,random_state=11)\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer(analyzer= 'char', ngram_range=(2,5))\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "#joblib.dump(vectorizer, filename_vec)\n",
    "\n",
    "\n",
    "## Deletion of reapeted columns.\n",
    "variance_filtter = VarianceThreshold()\n",
    "variance_filtter.fit(X_train, y_train)\n",
    "#joblib.dump(variance_filtter, filename_variance)\n",
    "X_train = variance_filtter.transform(X_train)\n",
    "X_test = variance_filtter.transform(X_test)\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "    # TRAINING SVM CLASSIFIER\n",
    "\n",
    "ClassifierSVM =  SVC(C = 23.0, gamma= 0.41, shrinking=True, kernel='rbf', class_weight=None, probability=False)\n",
    "ClassifierSVM.fit(X_train, y_train)\n",
    "#joblib.dump(ClassifierSVM, filename_svm)\n",
    "print('Done!')\n",
    "\n",
    "\n",
    "# Classification scores and metrics.\n",
    "print(\"\\n\\n#############################################################################################################\")\n",
    "print(\"\\nClassifier Scores:\\n\")\n",
    "print(\"Precision (f1 Score):\")\n",
    "svm_predict = ClassifierSVM.predict(X_test)\n",
    "class_score = f1_score(y_test, svm_predict, average=None)\n",
    "print(\"Negative Class: %\", class_score[0], \"\\nPositve Class: %\", class_score[1])\n",
    "print(\"\\nConfusion Matrix: \\n \",confusion_matrix(y_test, svm_predict),'\\n\\n')\n",
    "print(classification_report(y_test, svm_predict))\n",
    "print('\\n\\nAditional Info:')\n",
    "print('Number of suport Vectors for each class: ', ClassifierSVM.n_support_)\n",
    "score = f1_score(y_test, svm_predict, pos_label='TABLE', average='binary')\n",
    "matw = f1_score(y_test, svm_predict, pos_label='NONE TABLE', average='binary')\n",
    "f1scores.append(score)\n",
    "mathewscores.append(matw)\n",
    "\n",
    "print(\"\\n\\n\\n----------------------------------- Random Forest -----------------------------------------\")\n",
    "\n",
    "\n",
    "ClassifierRF =  RandomForestClassifier(bootstrap=False,n_estimators=100,criterion='gini',class_weight='balanced')\n",
    "ClassifierRF.fit(X_train, y_train)\n",
    "#joblib.dump(ClassifierSVM, filename_svm)\n",
    "print('Done!\\n')\n",
    "\n",
    "\n",
    "# Classification scores and metrics.\n",
    "print(\"#############################################################################################################\")\n",
    "print(\"Classifier Scores:\\n\")\n",
    "print(\"Precision (f1 Score):\")\n",
    "rf_predict = ClassifierRF.predict(X_test)\n",
    "class_score = f1_score(y_test, rf_predict, average=None)\n",
    "print(\"Negative Class: %\", class_score[0], \"\\nPositve Class: %\", class_score[1])\n",
    "print(\"\\nConfusion Matrix: \\n \",confusion_matrix(y_test, rf_predict),'\\n\\n')\n",
    "print(classification_report(y_test, rf_predict))\n",
    "score = f1_score(y_test, rf_predict, pos_label='TABLE', average='binary')\n",
    "matw =  f1_score(y_test, svm_predict, pos_label='NONE TABLE', average='binary')\n",
    "f1scores.append(score)\n",
    "mathewscores.append(matw)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n----------------------------------- SGDClassifier  -----------------------------------------\")\n",
    "\n",
    "ClassifierSGDC =  SGDClassifier(alpha=1e-06,class_weight=None,epsilon=0.1,l1_ratio=0.15,loss='log',penalty='l1')\n",
    "ClassifierSGDC.fit(X_train, y_train)\n",
    "#joblib.dump(ClassifierSVM, filename_svm)\n",
    "print('Done!\\n')\n",
    "\n",
    "\n",
    "# Classification scores and metrics.\n",
    "print(\"\\n\\n#############################################################################################################\")\n",
    "print(\"\\nClassifier Scores:\\n\")\n",
    "print(\"Precision (f1 Score):\")\n",
    "sgdc_predict = ClassifierSGDC.predict(X_test)\n",
    "class_score = f1_score(y_test, sgdc_predict, average=None)\n",
    "print(\"Negative Class: %\", class_score[0], \"\\nPositve Class: %\", class_score[1])\n",
    "print(\"\\nConfusion Matrix: \\n \",confusion_matrix(y_test, sgdc_predict),'\\n\\n')\n",
    "print(classification_report(y_test, sgdc_predict))\n",
    "score = f1_score(y_test, sgdc_predict, pos_label='TABLE', average='binary')\n",
    "matw =  f1_score(y_test, svm_predict, pos_label='NONE TABLE', average='binary')\n",
    "f1scores.append(score)\n",
    "mathewscores.append(matw)\n",
    "\n",
    "\n",
    "\n",
    "all_scores['BestRun'] = f1scores\n",
    "all_matew[f'BestRun'] = mathewscores\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = ['SVC','Random Forest','SGD Classifier']\n",
    "values = list(all_scores.values())[0]\n",
    "values3 = list(all_matew.values())[0]\n",
    "\n",
    "\n",
    "X_axis = np.arange(len(X))\n",
    "fig, ax = plt.subplots(figsize=(7, 6), layout='tight')\n",
    "ax.bar(X_axis - 0.15, values, 0.3, label = 'Table')\n",
    "ax.bar(X_axis + 0.15, values3, 0.3, label = 'None Table')\n",
    "ax.set_xticks(X_axis, X)\n",
    "ax.set_xlabel(\"Algorithms\",fontsize=13)\n",
    "ax.set_ylabel(\"F1-Score\",fontsize=13)\n",
    "ax.set_ylim(0,1.3)\n",
    "plt.plot([-0.4,1,2.4],[0.91,0.91,0.91], linestyle='-',c='black',linewidth=3)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Binarize the targets\n",
    "y_ = label_binarize(y_train, classes=['TABLES', 'NONE TABLE'])\n",
    "y_test_ = label_binarize(y_test, classes=['TABLES', 'NONE TABLE'])\n",
    "\n",
    "y_score = ClassifierSVM.decision_function(X_test)\n",
    "\n",
    "## Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "fpr, tpr, threshold = roc_curve(y_test_, y_score)\n",
    "roc_auc = auc(tpr,fpr)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( layout='constrained')\n",
    "ax.plot(tpr,fpr, label = f'AUC = {round(roc_auc,2)}')\n",
    "ax.legend(loc = 'lower right')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "fig.savefig('../../AUC_ROC.png')\n",
    "\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "\n",
    "average_precision = average_precision_score(y_test_, y_score)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "disp = plot_precision_recall_curve(ClassifierSVM, X_test, y_test)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('../../AUC-PR.png')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4), layout='tight')\n",
    "matrix = plot_confusion_matrix(ClassifierSVM, X_test, y_test,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true')\n",
    "plt.show()\n",
    "plt.savefig('../../ConfusionMatrix.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "51256fb29d3417d51258ec6b95047b6f4277a1591b5c1cc3248ea847bb42ce8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
