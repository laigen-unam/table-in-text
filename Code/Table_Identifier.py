'''
NAME
       Table Identifier RBF

VERSION
        2.0

AUTHOR
        Axel Zagal Norman <azagal@lcg.unam.mx> 
        Dante Torres Adonis <dtorres@lcg.unam.mx>
        Joel Rodriguez Herrera <joelrh@lcg.unam.mx>

DESCRIPTION
        Table identifier script

CATEGORY
        RBF SVM

USAGE
        python3 Table_Identifier.py -iJ '../table-in-text/NLP/TestJsonNLP_Files'

ARGUMENTS
        -iJ, --input_files_json: Path to directory were the CoreNLP json files are. RECOMENDED.

SEE ALSO
        The inputs files of this code are generated by Stanford Core NLP, for details of installation visit: https://stanfordnlp.github.io/CoreNLP/index.html
        Once the program is installed, run the following comands inside table-in-text directory: 

            1. $ export CLASSPATH=$CLASSPATH:/path/to/stanford-corenlp-4.2.0/*:
             
            2.  $ for file in - path/files/to_tokenize/ 
                $ do
                $ java -mx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma -outputFormat json -outputDirectory NLP/TestJsonNLP_Files/ -file "${file}" ;
                $ done

        Now  run this code with the comand specified in USAGE
'''

   
    # LIBRARIES

import joblib
import argparse
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import collections
import json 
import os 

#############################################################################################################


    # FUNCTIONS

def json2txt():
    ## Convert .json into .txt
    print('\n\nChanging format of CoreNLP files...')
    all_json_files = os.listdir(args.input_files_json) 
    for nlp_files in all_json_files:
        if nlp_files.startswith('.') == False:
            with open(f'{args.input_files_json}/{nlp_files}', 'r') as f:
                single_file = json.load(f)
            name = nlp_files.split('.')[0]
            text_nlp_file = open(f"../table-in-text/NLP/TestNLP_Files/{name}.txt", "+w")
            all_sentences = single_file['sentences'] 
            for sentence in all_sentences: 
                all_words = sentence['tokens'] 
                for line_words in all_words: 
                    nlp_text = (line_words['word'], "|", line_words['lemma'], "|", line_words['pos'],' ') 
                    text_nlp_file.write("".join(nlp_text)) 
                text_nlp_file.write('\n')
    print('Done!\n\n\n')



#############################################################################################################


    # COMMAND LINE ARGUMENT PASSING

parser = argparse.ArgumentParser(description="Finds tables in files, best if input is a .json file by CoreNLP")

## path to text file with cases of positive class processed with CoreNLP
parser.add_argument(
  "-iJ", "--input_files_json",
  metavar="path/to/json_files/dir",
  help="path to directory of json files",
  required=False)


args = parser.parse_args()


#############################################################################################################

    # FILE PREPROCESSING


### Storage directory of all required objects
classifier_rbf = joblib.load('../table-in-text/joblib/ClassifierSVM.joblib')
vectorizer = joblib.load('../table-in-text/joblib/Vectorizer.joblib')
sel_variance = joblib.load('../table-in-text/joblib/Variance.joblib')
sel_percentile = joblib.load('../table-in-text/joblib/Percentile.joblib')


json2txt()
## Extract POS of test files
num_tables = 0
files_no_tables = []
all_test_files = os.listdir("../table-in-text/NLP/TestNLP_Files") 
print('Extracting tables...')
for test_file in all_test_files:
    if test_file.startswith('.') == False:
        raw_sentences = []
        outPaths = []
        with open(f'../table-in-text/NLP/TestNLP_Files/{test_file}', 'r') as inFile:
            text = inFile.read()
        outPath = f'../table-in-text/NLP/TestPOS_Files/{test_file}'
        with open(outPath, 'w+') as outFile:
            newSentence = []
            textSentence = []
            for sentence in text.split('\n'):
                if len(sentence) == 0: continue
                newWords = []
                textWords = []
                for word in sentence.split(' '):
                    PartOfSpeech = word.split("|")
                    if len(PartOfSpeech) == 3:
                        newWords.append(f'{PartOfSpeech[2]}')
                        textWords.append(f'{PartOfSpeech[0]}')
                newSentence.append(' '.join(newWords))
                textSentence.append(' '.join(textWords))
            outFile.write('\n'.join(newSentence))

    

#############################################################################################################

    #  IDENTIFYING TABLES


        ## Transform features of each file
        sentences = open(outPath).readlines()
        text_data = vectorizer.transform(sentences)
        

        ## Feature Selection 
        data_set = sel_variance.transform(text_data)
        data_set = sel_percentile.transform(data_set)
        tableidentifier_rbf = classifier_rbf.predict(data_set)

        ## Only creates a result file if there is a table in the original file.
        thers_table = 0
        for answer in tableidentifier_rbf:
            if answer == 'SI':
                thers_table = 1

        ## Saves tables in new files
        if thers_table == 1:
            with open(f'../table-in-text/Results/{test_file}', '+w') as file:
                for index in range(len(tableidentifier_rbf)):
                    if tableidentifier_rbf[index] == 'SI':
                        num_tables += 1
                        file.write(textSentence[index])
                        file.write('\n\n')
        else:
            files_no_tables.append(test_file)

print('Done!\n\n\n')
print('Files containing only tables are available on: ../table-in-text/Results')
print('Number of tables found: ', num_tables)
print('Number of files wihtout tables: ', len(files_no_tables))
